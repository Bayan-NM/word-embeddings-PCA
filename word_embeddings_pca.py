# -*- coding: utf-8 -*-
"""word_embeddings_pca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JBvzWzAQgBCBu5-UtgVydtzli73eKNsb

###Word Embeddings PCA Visualization

This code reads in word embeddings from four different datasets: animals, countries, veggies, and fruits. It then performs PCA (Principal Component Analysis) on each dataset to reduce the dimensionality of the embeddings from 300 features to just 2. Finally, the reduced embeddings are plotted in a scatter plot with different colors for each category

###import library
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""###(1)load word embeddings representations from the data files."""

# List of data files
files = ["animals.csv", "countries.csv", "veggies.csv", "fruits.csv"]

# Load word embeddings from each file using pandas
embeddings = {}
for file in files:
    # Load data from file
    data = pd.read_csv(file, header=None, index_col=0, sep=' ')
    
    # Extract word vectors
    words = data.index.tolist()
    vectors = data.values

    # Store word vectors in dictionary
    embeddings[file.split(".")[0]] = {"words": words, "vectors": vectors}

"""###(2) Implement the PCA algorithm (step by step without using existing libraries) and represent words using two features (2D space)."""

# PCA Algorithm
def PCA(data, num_dimensions):
    # Normalize the data
    data_norm = (data - np.mean(data, axis=0)) / np.std(data, axis=0)
    
    # Calculate the covariance matrix
    cov_mat = np.cov(data_norm, rowvar=False)
    
    # Calculate the eigenvalues and eigenvectors of the covariance matrix
    eig_vals, eig_vecs = np.linalg.eig(cov_mat)
    
    # Sort the eigenvalues and eigenvectors in descending order
    sort_indices = np.argsort(eig_vals)[::-1]
    eig_vals_sorted = eig_vals[sort_indices]
    eig_vecs_sorted = eig_vecs[:, sort_indices]
    
    # Select the top k eigenvectors (k=num_dimensions)
    eig_vecs_topk = eig_vecs_sorted[:, :num_dimensions]
    
    # Project the data onto the selected eigenvectors to get the reduced representation
    data_reduced = np.dot(data_norm, eig_vecs_topk)
    
    return data_reduced

# Perform PCA on each dataset and store the reduced representation
reduced_embeddings = {}
for name, data in embeddings.items():
    reduced_embeddings[name] = PCA(data["vectors"], num_dimensions=2)

"""###(3) Plot a scatter figure to visualize the words that belong to four different categories."""

# Plot the reduced representation of each dataset
fig, ax = plt.subplots()

colors = {"animals": "red", "countries": "blue", "veggies": "green", "fruits": "orange"}

for name, data in reduced_embeddings.items():
    ax.scatter(data[:, 0], data[:, 1], color=colors[name], label=name)
ax.legend()
plt.title("Word Embeddings PCA Visualization")
plt.show()